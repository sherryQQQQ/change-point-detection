{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c93b439",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kalman filter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "import warnings\n",
    "from src.pmf import transient_distribution_piecewise\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import abc\n",
    "\n",
    "class RollingPredictor(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def rolling_prediction(self, times, values, verbose=True):\n",
    "        pass\n",
    "    @abc.abstractmethod\n",
    "    def get_summary_metrics(self):\n",
    "        pass\n",
    "    @abc.abstractmethod\n",
    "    def plot_rolling_predictions(self, original_times=None, original_values=None, \n",
    "                               show_confidence=True, show_errors=True):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "class RollingKalmanPredictor(RollingPredictor):\n",
    "    \"\"\"\n",
    "    Rolling Kalman Predictor\n",
    "    \n",
    "    For each time point t, use all historical data up to t to predict the value at t.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, min_history_length=5):\n",
    "        \"\"\"\n",
    "        Initialize the rolling predictor\n",
    "        \n",
    "        Args:\n",
    "            min_history_length: Minimum number of historical data points required for prediction\n",
    "        \"\"\"\n",
    "        self.min_history_length = min_history_length\n",
    " \n",
    "    def estimate_parameters_window(self, times, values,a,b):\n",
    "        \"\"\"\n",
    "        Estimate model parameters from a given time window\n",
    "        \n",
    "        Use moment estimation for fast parameter estimation\n",
    "        \"\"\"\n",
    "        dt = np.mean(np.diff(times)) if len(times) > 1 else 0.1\n",
    "        \n",
    "        # Long-term mean\n",
    "        # b = np.mean(values)\n",
    "        \n",
    "        # Mean reversion speed (from lag-1 autocorrelation)\n",
    "        # if len(values) > 2:\n",
    "        #     values_array = np.array(values)\n",
    "        #     corr = np.corrcoef(values_array[:-1], values_array[1:])[0, 1]\n",
    "        #     phi = max(0.05, min(0.95, corr))\n",
    "        #     a = max(0.01, (1 - phi) / dt)\n",
    "        # else:\n",
    "        #     a = 0.3\n",
    "        \n",
    "        # Volatility parameter\n",
    "        if len(values) > 3:\n",
    "            residuals = []\n",
    "            for i in range(1, len(values)):\n",
    "                predicted = values[i-1] + a * (b - values[i-1]) * dt\n",
    "                residual = values[i] - predicted\n",
    "                residuals.append(residual)\n",
    "            \n",
    "            if len(residuals) > 0:\n",
    "                residual_var = np.var(residuals)\n",
    "                sigma = np.sqrt(residual_var / (b * dt))\n",
    "                sigma = max(0.1, min(3.0, sigma))\n",
    "            else:\n",
    "                sigma = 1.0\n",
    "        else:\n",
    "            sigma = 1.0\n",
    "        \n",
    "        return a, b, sigma, dt\n",
    "    \n",
    "    def kalman_one_step_prediction(self, times, values):\n",
    "        \"\"\"\n",
    "        Use historical data for one-step prediction\n",
    "        \n",
    "        Args:\n",
    "            times: Historical time series\n",
    "            values: Historical observation sequence\n",
    "            \n",
    "        Returns:\n",
    "            dict: Contains prediction value, uncertainty, etc.\n",
    "        \"\"\"\n",
    "        if len(times) < 2:\n",
    "            return None\n",
    "        \n",
    "        # Estimate model parameters (only using historical data)\n",
    "        a,b= 0.3,80\n",
    "\n",
    "        a, b, sigma, dt = self.estimate_parameters_window(times, values,a,b)\n",
    "        # Initialize Kalman filter\n",
    "        Z_current = values[0]\n",
    "        P_current = 100.0\n",
    "        \n",
    "        # Update Kalman filter state using historical data\n",
    "        for i in range(1, len(values)):\n",
    "            # Prediction step\n",
    "            Z_pred = Z_current + a * (b - Z_current) * dt\n",
    "            F = 1 - a * dt\n",
    "            Q = sigma**2 * max(0.1, Z_current) * dt\n",
    "            P_pred = F * P_current * F + Q\n",
    "            \n",
    "            # Update step\n",
    "            observation = values[i]\n",
    "            H = 1.0\n",
    "            R = max(1.0, 0.1 * abs(observation))\n",
    "            S = H * P_pred * H + R\n",
    "            K = P_pred * H / S\n",
    "            \n",
    "            innovation = observation - Z_pred\n",
    "            Z_current = max(0.01, Z_pred + K * innovation)\n",
    "            P_current = (1 - K * H) * P_pred\n",
    "        \n",
    "        # Next step prediction based on current state\n",
    "        Z_next_pred = Z_current + a * (b - Z_current) * dt\n",
    "        F = 1 - a * dt\n",
    "        Q = sigma**2 * max(0.1, Z_current) * dt\n",
    "        P_next_pred = F * P_current * F + Q\n",
    "        \n",
    "        # Calculate confidence interval\n",
    "        std_pred = np.sqrt(P_next_pred)\n",
    "        conf_lower = Z_next_pred - 1.96 * std_pred\n",
    "        conf_upper = Z_next_pred + 1.96 * std_pred\n",
    "        \n",
    "        return {\n",
    "            'prediction': Z_next_pred,\n",
    "            'uncertainty': std_pred,\n",
    "            'confidence_lower': conf_lower,\n",
    "            'confidence_upper': conf_upper,\n",
    "            'parameters': {'a': a, 'b': b, 'sigma': sigma, 'dt': dt}\n",
    "        }\n",
    "    \n",
    "    def rolling_prediction(self, times, values, verbose=True):\n",
    "        \"\"\"\n",
    "        Execute rolling prediction: for each time point, use previous data to predict the value at that point\n",
    "        \n",
    "        Args:\n",
    "            times: Complete time series\n",
    "            values: Complete observation sequence\n",
    "            verbose: Whether to output detailed information\n",
    "            \n",
    "        Returns:\n",
    "            dict: Summary of prediction results\n",
    "        \"\"\"\n",
    "        # Validate input\n",
    "        if len(times) != len(values):\n",
    "            raise ValueError(\"times and values must have the same length\")\n",
    "        \n",
    "        if len(times) < self.min_history_length + 1:\n",
    "            raise ValueError(f\"At least {self.min_history_length + 1} data points are required\")\n",
    "        \n",
    "        # Ensure data is sorted by time\n",
    "        sorted_indices = np.argsort(times)\n",
    "        times = np.array(times)[sorted_indices]\n",
    "        values = np.array(values)[sorted_indices]\n",
    "        \n",
    "        # Reset results\n",
    "        self.results = {\n",
    "            'prediction_times': [],\n",
    "            'predictions': [],\n",
    "            'actual_values': [],\n",
    "            'prediction_errors': [],\n",
    "            'confidence_lower': [],\n",
    "            'confidence_upper': [],\n",
    "            'model_parameters': [],\n",
    "            'used_history_length': [],\n",
    "            'predicted_step_function': [],\n",
    "            'predicted_step_function_time_interval': []\n",
    "        }\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"=\"*60)\n",
    "            print(\"Rolling prediction\")\n",
    "            print(\"=\"*60)\n",
    "            print(f\"Total data points: {len(times)}\")\n",
    "            print(f\"Prediction points: {len(times) - self.min_history_length}\")\n",
    "            print(f\"Minimum history length: {self.min_history_length}\")\n",
    "        \n",
    "        # For each time point, predict the value at that point (starting from the (min_history_length+1)th point)\n",
    "        successful_predictions = 0\n",
    "        self.results['predicted_step_function'] = [values[:self.min_history_length].mean()]\n",
    "        self.results['predicted_step_function_time_interval'] = [times[self.min_history_length-1] - times[0]]\n",
    "        last_current_time = times[self.min_history_length-1]\n",
    "        for i in range(self.min_history_length, len(times)):\n",
    "            # Use times[0:i] and values[0:i] to predict values[i]\n",
    "            history_times = times[:i]\n",
    "            history_values = values[:i]\n",
    "            \n",
    "            # Current time point and actual value to be predicted\n",
    "            current_time = times[i]\n",
    "            actual_value = values[i]\n",
    "            \n",
    "            # Perform prediction (only using historical data)\n",
    "            pred_result = self.kalman_one_step_prediction(history_times, history_values)\n",
    "            \n",
    "            if pred_result is not None:\n",
    "                prediction = pred_result['prediction']\n",
    "                conf_lower = pred_result['confidence_lower']\n",
    "                conf_upper = pred_result['confidence_upper']\n",
    "                params = pred_result['parameters']\n",
    "                \n",
    "                # Store results\n",
    "                # if len(self.results['prediction_times']) == 1 or current_time != self.results['prediction_times'][-1]:\n",
    "                self.results['prediction_times'].append(current_time)\n",
    "                self.results['predicted_step_function'].append(prediction)\n",
    "                self.results['predicted_step_function_time_interval'].append(current_time -last_current_time)\n",
    "                last_current_time = current_time\n",
    "                \n",
    "                self.results['predictions'].append(prediction)\n",
    "                self.results['actual_values'].append(actual_value)\n",
    "                self.results['prediction_errors'].append(actual_value - prediction)\n",
    "                self.results['confidence_lower'].append(conf_lower)\n",
    "                self.results['confidence_upper'].append(conf_upper)\n",
    "                self.results['model_parameters'].append(params)\n",
    "                self.results['used_history_length'].append(len(history_times))\n",
    "                \n",
    "                successful_predictions += 1\n",
    "                \n",
    "                if verbose and i < self.min_history_length + 5:\n",
    "                    print(f\"Time {current_time:.3f}: Predicted={prediction:.2f}, Actual={actual_value:.2f}, \"\n",
    "                          f\"Error={actual_value - prediction:.2f}\")\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Successfully predicted: {successful_predictions} points\")\n",
    "        \n",
    "        return self.get_summary_metrics()\n",
    "    \n",
    "    def get_summary_metrics(self):\n",
    "        \"\"\"\n",
    "        Calculate prediction performance metrics\n",
    "        \"\"\"\n",
    "        if len(self.results['predictions']) == 0:\n",
    "            return None\n",
    "        \n",
    "        predictions = np.array(self.results['predictions'])\n",
    "        actuals = np.array(self.results['actual_values'])\n",
    "        errors = np.array(self.results['prediction_errors'])\n",
    "        \n",
    "        # Calculate various metrics\n",
    "        mse = np.mean(errors**2)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = np.mean(np.abs(errors))\n",
    "        mape = np.mean(np.abs(errors / actuals)) * 100\n",
    "        \n",
    "        # Direction accuracy (whether the predicted trend is correct)\n",
    "        if len(actuals) > 1:\n",
    "            actual_directions = np.sign(np.diff(actuals))\n",
    "            pred_directions = np.sign(np.diff(predictions))\n",
    "            direction_accuracy = np.mean(actual_directions == pred_directions) * 100\n",
    "        else:\n",
    "            direction_accuracy = 0\n",
    "        \n",
    "        # Confidence interval coverage\n",
    "        lower_bounds = np.array(self.results['confidence_lower'])\n",
    "        upper_bounds = np.array(self.results['confidence_upper'])\n",
    "        coverage = np.mean((actuals >= lower_bounds) & (actuals <= upper_bounds)) * 100\n",
    "        \n",
    "        return {\n",
    "            'mse': mse,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'mape': mape,\n",
    "            'direction_accuracy': direction_accuracy,\n",
    "            'confidence_coverage': coverage,\n",
    "            'n_predictions': len(predictions),\n",
    "            'mean_prediction': np.mean(predictions),\n",
    "            'mean_actual': np.mean(actuals)\n",
    "        }\n",
    "    \n",
    "    def plot_rolling_predictions(self, original_times=None, original_values=None, \n",
    "                               show_confidence=True, show_errors=True):\n",
    "        \"\"\"\n",
    "        Plot rolling prediction results\n",
    "        \n",
    "        Args:\n",
    "            original_times: Original complete time series (for context)\n",
    "            original_values: Original complete observation sequence\n",
    "            show_confidence: Whether to show confidence interval\n",
    "            show_errors: Whether to show error plot\n",
    "        \"\"\"\n",
    "        if len(self.results['predictions']) == 0:\n",
    "            print(\"No prediction results to plot\")\n",
    "            return\n",
    "        \n",
    "        n_plots = 2 if show_errors else 1\n",
    "        fig, axes = plt.subplots(n_plots, 1, figsize=(15, 6*n_plots))\n",
    "        if n_plots == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        # Main plot: Predicted vs Actual\n",
    "        ax1 = axes[0]\n",
    "        \n",
    "        # If original data is provided, plot the complete observation sequence as background\n",
    "        if original_times is not None and original_values is not None:\n",
    "            ax1.plot(original_times, original_values, 'lightgray', alpha=0.5, \n",
    "                    linewidth=1, label='Complete observation sequence')\n",
    "        \n",
    "        # Prediction and actual points\n",
    "        pred_times = self.results['prediction_times']\n",
    "        predictions = self.results['predictions']\n",
    "        actual_values = self.results['actual_values']\n",
    "        # Use line plots instead of scatter plots for actual and predicted values\n",
    "        ax1.plot(pred_times, actual_values, color='blue', alpha=0.7, linewidth=2, \n",
    "                 label='Actual values', zorder=3, marker='o')\n",
    "        ax1.plot(pred_times, predictions, color='red', alpha=0.8, linewidth=2, \n",
    "                 label='Predicted values', zorder=3, marker='^')\n",
    "        \n",
    "        # Connecting lines to show the relationship between predicted and actual values\n",
    "        for i in range(len(pred_times)):\n",
    "            ax1.plot([pred_times[i], pred_times[i]], \n",
    "                    [actual_values[i], predictions[i]], \n",
    "                    'gray', alpha=0.3, linewidth=1, zorder=1)\n",
    "        \n",
    "        # Confidence interval\n",
    "        if show_confidence:\n",
    "            conf_lower = self.results['confidence_lower']\n",
    "            conf_upper = self.results['confidence_upper']\n",
    "            ax1.fill_between(pred_times, conf_lower, conf_upper, \n",
    "                           alpha=0.2, color='red', label='95% Confidence interval')\n",
    "        \n",
    "        ax1.set_ylabel('Intensity value Z_t')\n",
    "        ax1.set_title('Rolling prediction result: predict each time point using historical data')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add performance metrics text\n",
    "        metrics = self.get_summary_metrics()\n",
    "        if metrics:\n",
    "            metrics_text = (f'RMSE: {metrics[\"rmse\"]:.3f}\\n'\n",
    "                           f'MAE: {metrics[\"mae\"]:.3f}\\n'\n",
    "                           f'Direction accuracy: {metrics[\"direction_accuracy\"]:.1f}%\\n'\n",
    "                           f'Confidence coverage: {metrics[\"confidence_coverage\"]:.1f}%')\n",
    "            ax1.text(0.02, 0.98, metrics_text, transform=ax1.transAxes, \n",
    "                    verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat'))\n",
    "        \n",
    "        # Error plot\n",
    "        if show_errors:\n",
    "            ax2 = axes[1]\n",
    "            errors = self.results['prediction_errors']\n",
    "            \n",
    "            ax2.scatter(pred_times, errors, color='purple', alpha=0.6, s=20)\n",
    "            ax2.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "            \n",
    "            # Error statistics line\n",
    "            error_mean = np.mean(errors)\n",
    "            error_std = np.std(errors)\n",
    "            ax2.axhline(y=error_mean, color='red', linestyle='-', alpha=0.7, \n",
    "                       label=f'Average error: {error_mean:.3f}')\n",
    "            ax2.axhline(y=error_mean + 2*error_std, color='red', linestyle=':', \n",
    "                       alpha=0.7, label='±2 Std')\n",
    "            ax2.axhline(y=error_mean - 2*error_std, color='red', linestyle=':', alpha=0.7)\n",
    "            \n",
    "            ax2.set_ylabel('Prediction error (Actual - Predicted)')\n",
    "            ax2.set_xlabel('Time')\n",
    "            ax2.set_title('Prediction error analysis')\n",
    "            ax2.legend()\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def get_prediction_at_time(self, target_time):\n",
    "        \"\"\"\n",
    "        Get prediction results at a specific time point\n",
    "        \"\"\"\n",
    "        for i, time in enumerate(self.results['prediction_times']):\n",
    "            if abs(time - target_time) < 1e-6:\n",
    "                return {\n",
    "                    'time': time,\n",
    "                    'prediction': self.results['predictions'][i],\n",
    "                    'actual': self.results['actual_values'][i],\n",
    "                    'error': self.results['prediction_errors'][i],\n",
    "                    'confidence_lower': self.results['confidence_lower'][i],\n",
    "                    'confidence_upper': self.results['confidence_upper'][i],\n",
    "                    'history_length': self.results['used_history_length'][i]\n",
    "                }\n",
    "        return None\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525300c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cpd\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "import statsmodels.api as sm\n",
    "\n",
    "class OLScpdPredictor():\n",
    "    def __init__(self, min_history_length=5):\n",
    "        self.results = {\n",
    "            # 'prediction_times': [],\n",
    "            'predicted_value': [],\n",
    "            'value': [],\n",
    "            'prediction_errors': [],\n",
    "            # 'confidence_lower': [],\n",
    "            # 'confidence_upper': [],\n",
    "            # 'model_parameters': [],\n",
    "            # 'used_history_length': [],\n",
    "            'stepwise_value': [],\n",
    "            'predicted_step_function_time_interval': []\n",
    "        }\n",
    "        \n",
    "        pass\n",
    "    def mmd_statistic(self,x, y, gamma=None):\n",
    "        \"\"\"\n",
    "        Compute the Maximum Mean Discrepancy (MMD) between two samples.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : array-like\n",
    "            First sample\n",
    "        y : array-like\n",
    "            Second sample\n",
    "        gamma : float\n",
    "            RBF kernel bandwidth parameter\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            MMD statistic value\n",
    "        \"\"\"\n",
    "        x = np.array(x).reshape(-1, 1)\n",
    "        y = np.array(y).reshape(-1, 1)\n",
    "        \n",
    "        # Compute kernel matrices\n",
    "        k_xx = rbf_kernel(x, x, gamma)\n",
    "        k_yy = rbf_kernel(y, y, gamma)\n",
    "        k_xy = rbf_kernel(x, y, gamma)\n",
    "        \n",
    "        # Compute MMD\n",
    "        n = len(x)\n",
    "        m = len(y)\n",
    "        \n",
    "        mmd = (np.sum(k_xx) - np.trace(k_xx)) / (n * (n - 1))\n",
    "        mmd += (np.sum(k_yy) - np.trace(k_yy)) / (m * (m - 1))\n",
    "        mmd -= 2 * np.sum(k_xy) / (n * m)\n",
    "        \n",
    "        return mmd\n",
    "\n",
    "    def rolling_prediction(self,data, window_size=3, gamma=None, plot=True, method='ols',file_name=None,cpd_method='mmd'):\n",
    "        \"\"\"\n",
    "        Analyze deviations between predicted model values and actual data using Maximum Mean Discrepancy (MMD).\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : pandas.DataFrame\n",
    "            DataFrame containing 'value' and 'time' columns.\n",
    "        window_size : int, optional (default=3)\n",
    "            Size of sliding window for analysis.\n",
    "        gamma : float, optional (default=None)\n",
    "            RBF kernel bandwidth parameter. If None, calculated using median heuristic.\n",
    "        plot : bool, optional (default=True)\n",
    "            Whether to plot the results.\n",
    "        method : str, optional (default='ols')\n",
    "            'ols' for standard linear regression on time, 'diff_ols' for the mean-reverting model.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.DataFrame\n",
    "            DataFrame with original data, predicted values, and deviation points detected.\n",
    "        list\n",
    "            List of detected significant deviation indices.\n",
    "        \"\"\"\n",
    "        data = data.copy()\n",
    "\n",
    "        if 'time' not in data.columns:\n",
    "            data['time'] = np.arange(len(data))\n",
    "        \n",
    "        if 'predicted_value' not in data.columns:\n",
    "            data['predicted_value'] = np.nan\n",
    "\n",
    "        all_deviation_points = []\n",
    "        \n",
    "        if len(data) < 2 * window_size:\n",
    "            print(f\"Warning: Data length ({len(data)}) is less than 2*window_size ({2*window_size})\")\n",
    "            return data, all_deviation_points, np.nan\n",
    "\n",
    "        detect_window_size = int(window_size * 1)\n",
    "        predict_window_size = 2 * window_size - detect_window_size\n",
    "\n",
    "        def fit_diff_ols(df_window):\n",
    "            if len(df_window) < 2:\n",
    "                return None\n",
    "            \n",
    "            delta_t = df_window['time'].diff().mean()\n",
    "            if pd.isna(delta_t) or delta_t == 0:\n",
    "                delta_t = 1.0\n",
    "                \n",
    "            y_series = df_window['value'].diff() / delta_t\n",
    "            x_series = df_window['value'].shift(1)\n",
    "            \n",
    "            temp_df = pd.DataFrame({'y': y_series, 'x': x_series}).dropna()\n",
    "\n",
    "            if len(temp_df) < 2:\n",
    "                return None\n",
    "                \n",
    "            Y = temp_df['y']\n",
    "            X = sm.add_constant(temp_df['x'])\n",
    "            \n",
    "            return sm.OLS(Y, X).fit()\n",
    "\n",
    "        # --- Initial Model Training ---\n",
    "        initial_window = data.iloc[:window_size]\n",
    "        if method == 'ols':\n",
    "            X_init = sm.add_constant(initial_window['time'])\n",
    "            Y_init = initial_window['value']\n",
    "            results = sm.OLS(Y_init, X_init).fit()\n",
    "        elif method == 'diff_ols':\n",
    "            results = fit_diff_ols(initial_window)\n",
    "            if results is None:\n",
    "                print(\"Could not initialize diff_ols model due to insufficient data.\")\n",
    "                return data, [], np.nan\n",
    "        \n",
    "        # --- Prediction and Detection Loop ---\n",
    "        for i in range(2 * window_size, len(data) + 1, window_size):\n",
    "            predict_start_idx = i - window_size\n",
    "            predict_end_idx = predict_start_idx + predict_window_size\n",
    "            predict_indices = data.index[predict_start_idx:predict_end_idx]\n",
    "            \n",
    "            # --- Windowed Prediction ---\n",
    "            if method == 'ols':\n",
    "                X_predict = sm.add_constant(data.loc[predict_indices, 'time'])\n",
    "                data.loc[predict_indices, 'predicted_value'] = results.predict(X_predict)\n",
    "            elif method == 'diff_ols':\n",
    "                beta_0, beta_1 = results.params\n",
    "                a = -beta_1\n",
    "                b = -beta_0 / beta_1 if abs(beta_1) > 1e-6 else np.mean(data['value'].iloc[:i-window_size])\n",
    "                \n",
    "                z0_idx = predict_start_idx - 1 if predict_start_idx > 0 else 0\n",
    "                z0 = data['value'].iloc[z0_idx]\n",
    "                t0 = data['time'].iloc[z0_idx]\n",
    "                time_diffs = data.loc[predict_indices, 'time'] - t0\n",
    "                predicted_values = b + (z0 - b) * np.exp(- a * time_diffs)\n",
    "                \n",
    "                # Set an upper limit for predicted values\n",
    "                upper_limit = np.max(data['value'].iloc[:i-window_size])  # You can adjust this logic as needed\n",
    "                predicted_values = np.minimum(predicted_values, upper_limit)\n",
    "                lower_limit = np.min(data['value'].iloc[:i-window_size])\n",
    "                predicted_values = np.maximum(predicted_values, lower_limit)\n",
    "                data.loc[predict_indices, 'predicted_value'] = predicted_values\n",
    "            data.loc[predict_indices, 'stepwise_value'] = data.loc[predict_indices, 'predicted_value'].mean()\n",
    "            \n",
    "            # --- Distributional Comparison with MMD ---\n",
    "            detect_indices = data.index[i - window_size : i - window_size + detect_window_size]\n",
    "            if cpd_method == 'mmd':\n",
    "                prob = self.mmd_statistic(data.loc[detect_indices, 'value'], data.loc[detect_indices, 'predicted_value'])\n",
    "            elif cpd_method == 'ks':\n",
    "                prob = self.ks_statistic(data.loc[detect_indices, 'value'], data.loc[detect_indices, 'predicted_value'])\n",
    "            threshold = 0.05\n",
    "            \n",
    "            # --- Deviation Detection & Model Adaptation ---\n",
    "            if prob > threshold:\n",
    "                all_deviation_points.append(i - window_size)\n",
    "                # --- Model Adaptation upon Deviation ---\n",
    "                refit_window = data.iloc[i - window_size : i - window_size + detect_window_size]\n",
    "                if method == 'ols':\n",
    "                    X_refit = sm.add_constant(refit_window['time'])\n",
    "                    Y_refit = refit_window['value']\n",
    "                    results = sm.OLS(Y_refit, X_refit).fit()\n",
    "                elif method == 'diff_ols':\n",
    "                    new_results = fit_diff_ols(refit_window)\n",
    "                    if new_results:\n",
    "                        results = new_results\n",
    "            else:\n",
    "                # --- Continuous Learning (No Deviation) ---\n",
    "                last_dev_idx = all_deviation_points[-1] if all_deviation_points else 0\n",
    "                refit_window = data.iloc[last_dev_idx:i]\n",
    "                if method == 'ols':\n",
    "                    X_refit = sm.add_constant(refit_window['time'])\n",
    "                    Y_refit = refit_window['value']\n",
    "                    results = sm.OLS(Y_refit, X_refit).fit()\n",
    "                elif method == 'diff_ols':\n",
    "                    new_results = fit_diff_ols(refit_window)\n",
    "                    if new_results:\n",
    "                        results = new_results\n",
    "\n",
    "        # --- Initial Data Handling ---\n",
    "        data.loc[data.index[:window_size], 'stepwise_value'] = data.loc[data.index[:window_size], 'value'].mean()\n",
    "        data.loc[data.index[:window_size], 'predicted_value'] = data.loc[data.index[:window_size], 'value'].mean()\n",
    "\n",
    "        # --- Plotting and Final Metrics ---\n",
    "\n",
    "        # Plot the results if requested\n",
    "        if plot and len(data) > 0:\n",
    "            plt.figure(figsize=(12, 7))\n",
    "            plt.plot(data['time'], data['value'], label='Original Data', color='blue')\n",
    "            plt.plot(data['time'], data['predicted_value'], label='Predicted Values', \n",
    "                    color='green', linestyle='--', alpha=0.8)\n",
    "            plt.step(data['time'], data['stepwise_value'], label='Stepwise Function', color='orange', where='post', linewidth=2)\n",
    "\n",
    "            plt.xlim(0, data['time'].iloc[-1])\n",
    "            plt.title(f'Time Series with Detected Prediction Deviations, file_name={file_name},method={method}')\n",
    "            plt.xlabel('Time')\n",
    "            plt.ylabel('Value')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        self.results['prediction_times'] = data['time']\n",
    "        self.results['predictions'] = data['predicted_value']\n",
    "        self.results['actual_values'] = data['value']\n",
    "        self.results['prediction_errors'] = data['predicted_value'] - data['value']\n",
    "        self.results['predicted_step_function'] = data['stepwise_value']\n",
    "        self.results['predicted_step_function_time_interval'] = data['time']-data['time'].shift(1)\n",
    "        self.results['predicted_step_function_time_interval'].iloc[0] = data['time'].iloc[0]\n",
    "\n",
    "        return self.get_summary_metrics()\n",
    "    \n",
    "    def get_summary_metrics(self):\n",
    "        \"\"\"\n",
    "        Calculate prediction performance metrics\n",
    "        \"\"\"\n",
    "        if len(self.results['predictions']) == 0:\n",
    "            return None\n",
    "        if 'prediction_errors' not in self.results:\n",
    "            self.results['prediction_errors'] = self.results['predictions'] - self.results['actual_values']\n",
    "        if 'confidence_lower' not in self.results:\n",
    "            self.results['confidence_lower'] = self.results['predictions'] - 1.96 * np.sqrt(self.results['prediction_errors']**2)\n",
    "        if 'confidence_upper' not in self.results:\n",
    "            self.results['confidence_upper'] = self.results['predictions'] + 1.96 * np.sqrt(self.results['prediction_errors']**2)\n",
    "        predictions = np.array(self.results['predictions'].dropna())\n",
    "        actuals = np.array(self.results['actual_values'].dropna())\n",
    "        errors = np.array(self.results['prediction_errors'].dropna())\n",
    "        \n",
    "        # Calculate various metrics\n",
    "        mse = np.mean(errors**2)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = np.mean(np.abs(errors))\n",
    "        mape = np.mean(np.abs(errors / actuals[1:])) * 100\n",
    "        \n",
    "        # Direction accuracy (whether the predicted trend is correct)\n",
    "        if len(actuals) > 1:\n",
    "            actual_directions = np.sign(np.diff(actuals[1:]))\n",
    "            pred_directions = np.sign(np.diff(predictions))\n",
    "            direction_accuracy = np.mean(actual_directions == pred_directions) * 100\n",
    "        else:\n",
    "            direction_accuracy = 0\n",
    "        \n",
    "        # Confidence interval coverage\n",
    "        lower_bounds = np.array(self.results['confidence_lower'])\n",
    "        upper_bounds = np.array(self.results['confidence_upper'])\n",
    "        coverage = np.mean((actuals >= lower_bounds) & (actuals <= upper_bounds)) * 100\n",
    "        \n",
    "        return {\n",
    "            'mse': mse,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'mape': mape,\n",
    "            'direction_accuracy': direction_accuracy,\n",
    "            'confidence_coverage': coverage,\n",
    "            'n_predictions': len(predictions),\n",
    "            'mean_prediction': np.mean(predictions),\n",
    "            'mean_actual': np.mean(actuals)\n",
    "        }\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3a6791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "    \n",
    "from typing import ValuesView\n",
    "\n",
    "\n",
    "def rolling_predictor(times, values, min_history_length=5, verbose=True,method='cpd',window_size=3):\n",
    "    \"\"\"\n",
    "    Main function: rolling Kalman filter prediction\n",
    "    \n",
    "    For each time point t (starting from the (min_history_length+1)th point), use historical data up to t to predict the value at t.\n",
    "    \n",
    "    Args:\n",
    "        times: Time series\n",
    "        values: Observation sequence  \n",
    "        min_history_length: Minimum number of historical data points required for prediction\n",
    "        verbose: Whether to display detailed information\n",
    "        \n",
    "    Returns:\n",
    "        dict: Contains all prediction results and performance metrics\n",
    "    \"\"\"\n",
    "\n",
    "    if method == 'kalman':\n",
    "        predictor = RollingKalmanPredictor(min_history_length=min_history_length)\n",
    "        summary_metrics = predictor.rolling_prediction(times, values, verbose=verbose)\n",
    "        predictor.plot_rolling_predictions(original_times=times, original_values=values)\n",
    "\n",
    "    elif method == 'cpd':\n",
    "        predictor = OLScpdPredictor(min_history_length=min_history_length)\n",
    "        df = pd.DataFrame({'time': times, 'value': values})\n",
    "        summary_metrics = predictor.rolling_prediction(df,window_size=window_size, gamma=None, plot=True, method='ols',file_name=None)\n",
    "    \n",
    "    if verbose and summary_metrics:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Prediction performance summary\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Root mean square error (RMSE): {summary_metrics['rmse']:.4f}\")\n",
    "        print(f\"Mean absolute error (MAE): {summary_metrics['mae']:.4f}\")\n",
    "        print(f\"Mean absolute percentage error (MAPE): {summary_metrics['mape']:.2f}%\")\n",
    "        print(f\"Direction prediction accuracy: {summary_metrics['direction_accuracy']:.1f}%\")\n",
    "        print(f\"95% confidence interval coverage: {summary_metrics['confidence_coverage']:.1f}%\")\n",
    "        print(f\"Successfully predicted points: {summary_metrics['n_predictions']}\")\n",
    "        print(\"=\"*60)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return {\n",
    "        'predictor': predictor,\n",
    "        'summary_metrics': summary_metrics,\n",
    "        'detailed_results': predictor.results\n",
    "    }\n",
    "    \n",
    "def plot_pmf_overlap(window_sizes, t, file_path, save_path, Z_piece, dt_piece, mu, m,\n",
    "                    z_initial=80, hist_data_path=None, N=100, show_histogram=True):\n",
    "    \"\"\"\n",
    "    Plot PMFs for different window sizes overlapped with simulation histogram.\n",
    "    \n",
    "    Args:\n",
    "        window_sizes: List of window sizes to compare\n",
    "        t: Time point for PMF calculation\n",
    "        file_path: Source file path for reference (not used in current implementation)\n",
    "        save_path: Directory to save the plot\n",
    "        Z_piece: Predicted step function values\n",
    "        dt_piece: Time intervals for step function\n",
    "        mu: Model parameter mu (service rate)\n",
    "        m: Model parameter m\n",
    "        z_initial: Initial queue length (used for histogram file naming)\n",
    "        hist_data_path: Path to histogram data directory\n",
    "        N: Maximum number of states for PMF calculation\n",
    "        show_histogram: Whether to overlay simulation histogram\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    \n",
    "    # Define colors for theory curves\n",
    "    theory_colors = ['red', 'darkgreen', 'orange', 'purple', 'brown']\n",
    "    \n",
    "    # counts = None\n",
    "    x_hist = None\n",
    "    kl_data=[]\n",
    "    if show_histogram and hist_data_path:\n",
    "        try:\n",
    "            service_rate = 10 if z_initial == 5 else 100 if z_initial == 80 else mu\n",
    "            hist_filename = f\"for_histogram_CoxM1_Z0{z_initial}_serv{service_rate}_t{int(t)}.pickle\"\n",
    "            hist_path = os.path.join(hist_data_path, hist_filename)\n",
    "            \n",
    "            with open(hist_path, 'rb') as f:\n",
    "                hist_data = pickle.load(f)\n",
    "            \n",
    "            counts = hist_data['counts']\n",
    "            bins = hist_data['bins']\n",
    "   \n",
    "           \n",
    "            if bins[0] != 0:\n",
    "                bins = np.array(bins, dtype=int)\n",
    "                interval = max(int(bins[1] - bins[0]), 1)\n",
    "                bins_before = np.arange(0, bins[0], interval)\n",
    "                bins = np.concatenate([bins_before, bins])\n",
    "                counts_before = np.zeros(len(bins_before))\n",
    "                counts = np.concatenate([counts_before, counts])\n",
    "            \n",
    "            \n",
    "            # Normalize counts to counts\n",
    "            \n",
    "            total_count = np.sum(counts)\n",
    "            if total_count > 0:\n",
    "                counts = counts / total_count\n",
    "                x_hist = bins[:-1]  # Use bin starts for x coordinates\n",
    "                \n",
    "                # Plot histogram\n",
    "            ax.bar(bins[:-1], counts,\n",
    "                    width=bins[1] - bins[0] if len(bins) > 1 else 1,\n",
    "                    alpha=0.4,\n",
    "                    color='lightblue',\n",
    "                    label=f'Simulation t={int(t)}',\n",
    "                    zorder=1)\n",
    "            print(f\"✓ Loaded histogram: {hist_filename}\")\n",
    "            print(f\"  Total count: {total_count}\")\n",
    "            print(f\"  Probability sum: {np.sum(counts):.6f}\")\n",
    "            print(f\"  Histogram range: [{counts.min():.6f}, {counts.max():.6f}]\")\n",
    "            print(f\"  Bins range: [{bins[0]}, {bins[-1]}]\")\n",
    "            print(f'length of bins: {len(bins)}, length of counts: {len(counts)}')\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: Histogram file not found: {hist_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading histogram: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    plotted_theory_curves = 0\n",
    "    \n",
    "    for idx, ws in enumerate(window_sizes):\n",
    "        try:\n",
    "            pt = transient_distribution_piecewise(Z_piece, dt_piece, mu, m, t=t, N=N)\n",
    "            if pt is None or len(pt) == 0:\n",
    "                print(f\"Warning: Empty PMF for window size {ws}\")\n",
    "                continue\n",
    "            \n",
    "\n",
    "            x_vals = np.arange(N)\n",
    "            y_vals = pt[:N]\n",
    "            # Determine color\n",
    "            color = theory_colors[idx] if idx < len(theory_colors) else f'C{idx}'\n",
    "            \n",
    "            # Plot PMF curve\n",
    "            ax.plot(x_vals, y_vals,\n",
    "                   marker='o',\n",
    "                   linestyle='-',\n",
    "                   color=color,\n",
    "                   label=f'Theory WS={ws}',\n",
    "                   linewidth=2,\n",
    "                   markersize=4,\n",
    "                   zorder=2)\n",
    "            \n",
    "            print(f\"✓ Plotted PMF for window size {ws}\")\n",
    "            print(f\"  PMF sum: {np.sum(pt):.6f}\")\n",
    "            print(f\"  Plotted PMF sum: {np.sum(y_vals):.6f}\")\n",
    "            print(f\"  PMF range: [{np.min(y_vals):.6f}, {np.max(y_vals):.6f}]\")\n",
    "            print(f\"  PMF length: {len(pt)}, Plotted length: {len(y_vals)}\")\n",
    "            \n",
    "            plotted_theory_curves += 1\n",
    "            \n",
    "\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating PMF for window size {ws}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "    # Configure plot appearance\n",
    "    if plotted_theory_curves == 0:\n",
    "        print(\"Warning: No theory curves were plotted\")\n",
    "    \n",
    "    # Set plot properties\n",
    "    ax.set_title(f'Queue Length Distribution: Cox/M/1\\n'\n",
    "                f't={int(t)}, Z₀={z_initial}, μ={mu}',\n",
    "                fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Queue Length (Number of Customers)', fontsize=12)\n",
    "    ax.set_ylabel('Probability', fontsize=12)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(loc='upper right')\n",
    "    save_dir = Path(save_path)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    filename = f\"pmf_overlap_t{int(t)}.png\"\n",
    "    full_path = save_dir / filename\n",
    "   \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(full_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "    new_bin_indices = np.floor(bin_centers + 0.5).astype(int)\n",
    "    max_new_bin = new_bin_indices.max()\n",
    "    new_counts = np.zeros(max_new_bin + 1)\n",
    "    for i, idx in enumerate(new_bin_indices):\n",
    "        if 0 <= idx <= max_new_bin:\n",
    "            new_counts[idx] += counts[i]\n",
    "    new_bins = np.arange(max_new_bin + 2)\n",
    "    bins = new_bins\n",
    "    counts = new_counts\n",
    "        \n",
    "    if bins[-1] < N:\n",
    "        bins_after = np.arange(bins[-1] + 1, N + 1)\n",
    "        counts_after = np.zeros(len(bins_after))\n",
    "        counts = np.concatenate([counts, counts_after])\n",
    "        bins = np.concatenate([bins, bins_after])\n",
    "        \n",
    "    kl_data.append(compare_pmfs_kl(counts, y_vals, labels=(\"Simulation\", \"Prediction\"), plot=True))\n",
    "    \n",
    " \n",
    "    \n",
    "    print(f\"✅ PMF overlap plot saved to {full_path}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "\n",
    "def calculate_kl_divergence(p: np.ndarray, q: np.ndarray, epsilon: float = 1e-10) -> float:\n",
    "    \"\"\"\n",
    "    Calculate KL divergence KL(P||Q) between two probability distributions.\n",
    "    \n",
    "    KL(P||Q) = sum(p_i * log(p_i / q_i))\n",
    "    \n",
    "    Args:\n",
    "        p: True/reference distribution (must sum to 1)\n",
    "        q: Approximating distribution (must sum to 1)\n",
    "        epsilon: Small value to avoid log(0) and division by zero\n",
    "        \n",
    "    Returns:\n",
    "        KL divergence value (always >= 0)\n",
    "    \"\"\"\n",
    "    # Ensure inputs are numpy arrays\n",
    "    p = np.asarray(p, dtype=float)\n",
    "    q = np.asarray(q, dtype=float)\n",
    "    \n",
    "    # Normalize to ensure they sum to 1\n",
    "    p = p / np.sum(p)\n",
    "    q = q / np.sum(q)\n",
    "    \n",
    "    # Make distributions the same length by padding with zeros\n",
    "    max_len = max(len(p), len(q))\n",
    "    if len(p) < max_len:\n",
    "        p = np.pad(p, (0, max_len - len(p)), mode='constant', constant_values=0)\n",
    "    if len(q) < max_len:\n",
    "        q = np.pad(q, (0, max_len - len(q)), mode='constant', constant_values=0)\n",
    "    \n",
    "    # Add epsilon to avoid numerical issues\n",
    "    p = p + epsilon\n",
    "    q = q + epsilon\n",
    "    \n",
    "    # Renormalize after adding epsilon\n",
    "    p = p / np.sum(p)\n",
    "    q = q / np.sum(q)\n",
    "    \n",
    "    # Calculate KL divergence\n",
    "    kl_div = np.sum(p * np.log(p / q))\n",
    "    \n",
    "    return kl_div\n",
    "\n",
    "\n",
    "def compare_pmfs_kl(pmf1: [np.ndarray, list], \n",
    "                    pmf2: [np.ndarray, list],\n",
    "                    labels:[str, str] = (\"PMF1\", \"PMF2\"),\n",
    "                    plot: bool = True):\n",
    "    \"\"\"\n",
    "    Compare two PMFs using KL divergence and optionally visualize them.\n",
    "    \n",
    "    Args:\n",
    "        pmf1: First probability mass function\n",
    "        pmf2: Second probability mass function\n",
    "        labels: Labels for the two PMFs\n",
    "        plot: Whether to plot the PMFs for comparison\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing KL divergences and statistics\n",
    "    \"\"\"\n",
    "     \n",
    " \n",
    "    \n",
    "    # Calculate both directions of KL divergence\n",
    "    kl_1_to_2 = calculate_kl_divergence(pmf1, pmf2)\n",
    "    kl_2_to_1 = calculate_kl_divergence(pmf2, pmf1)\n",
    "    \n",
    "    # Calculate symmetric KL (average of both directions)\n",
    "    kl_symmetric = (kl_1_to_2 + kl_2_to_1) / 2\n",
    "    \n",
    "    # Calculate Jensen-Shannon divergence (symmetric and bounded)\n",
    "    m = (pmf1 + pmf2) / 2\n",
    "    js_divergence = (calculate_kl_divergence(pmf1, m) + calculate_kl_divergence(pmf2, m)) / 2\n",
    "    \n",
    "    results = {\n",
    "        f'KL({labels[0]}||{labels[1]})': kl_1_to_2,\n",
    "        f'KL({labels[1]}||{labels[0]})': kl_2_to_1,\n",
    "        'KL_symmetric': kl_symmetric,\n",
    "        'JS_divergence': js_divergence,\n",
    "        'pmf1_entropy': -np.sum(pmf1 * np.log(pmf1 + 1e-10)),\n",
    "        'pmf2_entropy': -np.sum(pmf2 * np.log(pmf2 + 1e-10))\n",
    "    }\n",
    "    \n",
    "    if plot:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        \n",
    "        # Plot PMFs\n",
    "        ax1 = axes[0]\n",
    "        x1 = np.arange(len(pmf1))\n",
    "        x2 = np.arange(len(pmf2))\n",
    "\n",
    "        ax1.plot(x1, pmf1, 'b-o', label=labels[0], alpha=0.7, markersize=4)\n",
    "        ax1.plot(x2, pmf2, 'r-s', label=labels[1], alpha=0.7, markersize=4)\n",
    "        ax1.set_xlabel('State')\n",
    "        ax1.set_ylabel('Probability')\n",
    "        ax1.set_title('PMF Comparison')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        ax2 = axes[1]\n",
    "        pmf1_nonzero = pmf1[pmf1 > 0]\n",
    "        pmf2_nonzero = pmf2[pmf2 > 0]\n",
    "        x1_nonzero = x1[pmf1 > 0]\n",
    "        x2_nonzero = x2[pmf2 > 0]\n",
    "        \n",
    "        ax2.semilogy(x1_nonzero, pmf1_nonzero, 'b-o', label=labels[0], alpha=0.7, markersize=4)\n",
    "        ax2.semilogy(x2_nonzero, pmf2_nonzero, 'r-s', label=labels[1], alpha=0.7, markersize=4)\n",
    "        ax2.set_xlabel('State')\n",
    "        ax2.set_ylabel('Probability (log scale)')\n",
    "        ax2.set_title('PMF Comparison (Log Scale)')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        info_text = (f'KL({labels[0]}||{labels[1]}) = {kl_1_to_2:.4f}\\n'\n",
    "                    f'KL({labels[1]}||{labels[0]}) = {kl_2_to_1:.4f}\\n'\n",
    "                    f'Symmetric KL = {kl_symmetric:.4f}\\n'\n",
    "                    f'JS Divergence = {js_divergence:.4f}')\n",
    "        \n",
    "        fig.text(0.5, 0.02, info_text, ha='center', fontsize=10,\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ebcb55",
   "metadata": {},
   "source": [
    "# Z0=5,t=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c213e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #Parameters  for prediction\n",
    "    z_initial=5\n",
    "    min_history_length=10\n",
    "    ws=5\n",
    "    method='cpd'\n",
    "    t=1\n",
    "    values = pd.read_csv(f'/home/oxj7986/Cox_Arrivals/change-point-detection/data_integrated/arrival_data/initial_value_{z_initial}_samples_500.csv')['value']\n",
    "    times = pd.read_csv(f'/home/oxj7986/Cox_Arrivals/change-point-detection/data_integrated/arrival_data/initial_value_{z_initial}_samples_500.csv')['time']\n",
    "    print(f\"{method} Predictor\")\n",
    "    result = rolling_predictor(times, values, min_history_length=min_history_length, verbose=True,method=method,window_size=ws)\n",
    "    file_path_prediction = f'/home/oxj7986/Cox_Arrivals/change-point-detection/data_integrated/result_data/initial_value_{z_initial}_samples_500_predicted_{method}_min_history_length_{min_history_length}_ws_{ws}.csv'\n",
    "    pd.DataFrame(result).to_csv(file_path_prediction)\n",
    "    print(f\"\\nUsage:\")\n",
    "    print(f\"result = rolling_kalman_predictor(data['time'], data['value'])\")\n",
    "    print(f\"All prediction results in: result['detailed_results']\")\n",
    "    print('\\nPrinting PMF overlap:')\n",
    "    file_path_pmf = f'/home/oxj7986/Cox_Arrivals/change-point-detection/data_integrated/pmf_data/M=500/initial_value_{z_initial}_samples_500_predicted_{method}_pmf_min_hist_length_{min_history_length}/'   \n",
    "\n",
    "    print((result['detailed_results']['predicted_step_function_time_interval'].shape),(result['detailed_results']['predicted_step_function'].shape))\n",
    "    plot_pmf_overlap(\n",
    "        [ws],\n",
    "        t=t,\n",
    "        file_path=file_path_prediction,  # changed from 'file' to 'file_path'\n",
    "        save_path=file_path_pmf,\n",
    "        Z_piece=result['detailed_results']['predicted_step_function'],\n",
    "        dt_piece=result['detailed_results']['predicted_step_function_time_interval'],\n",
    "        mu=10 if z_initial == 5 else 100 if z_initial == 80 else 100,\n",
    "        m=1,\n",
    "        z_initial=z_initial,\n",
    "        hist_data_path='/home/oxj7986/Cox_Arrivals/change-point-detection/data_integrated/Simulation_histograms',\n",
    "        N=100, # max number of states\n",
    "        show_histogram=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cf2acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(42)\n",
    "    z_initial=5\n",
    "    min_history_length=10\n",
    "    method='kalman'\n",
    "    t=1\n",
    "    values = pd.read_csv(f'/home/oxj7986/Cox_Arrivals/change-point-detection/data_integrated/arrival_data/initial_value_{z_initial}_samples_500.csv')['value']\n",
    "    times = pd.read_csv(f'/home/oxj7986/Cox_Arrivals/change-point-detection/data_integrated/arrival_data/initial_value_{z_initial}_samples_500.csv')['time']\n",
    "    \n",
    "    print(f\"{method}_Predictor\")\n",
    " \n",
    "    # Execute rolling prediction\n",
    "\n",
    "    result = rolling_predictor(times, values, min_history_length=min_history_length, verbose=True,method=method)\n",
    "    file_path_prediction = f'/home/oxj7986/Cox_Arrivals/change-point-detection/data_integrated/result_data/initial_value_{z_initial}_samples_500_predicted_{method}_min_history_length_{min_history_length}.csv'\n",
    "    pd.DataFrame(result).to_csv(file_path_prediction)\n",
    "    \n",
    "    print(f\"\\nUsage:\")\n",
    "    print(f\"result = rolling_kalman_predictor(data['time'], data['value'])\")\n",
    "    print(f\"All prediction results in: result['detailed_results']\")\n",
    "    \n",
    "    print('\\nPrinting PMF overlap:')\n",
    "    file_path_pmf = f'/home/oxj7986/Cox_Arrivals/change-point-detection/data_integrated/pmf_data/M=500/initial_value_{z_initial}_samples_500_predicted_{method}_pmf_min_hist_length_{min_history_length}/'   \n",
    "\n",
    "\n",
    "    plot_pmf_overlap(\n",
    "        [min_history_length],\n",
    "        t=t,\n",
    "        file_path=file_path_prediction,   \n",
    "        save_path=file_path_pmf,\n",
    "        Z_piece=result['detailed_results']['predicted_step_function'],\n",
    "        dt_piece=result['detailed_results']['predicted_step_function_time_interval'],\n",
    "        mu=10 if z_initial == 5 else 100 if z_initial == 80 else 100,\n",
    "        m=1,\n",
    "        z_initial=z_initial,\n",
    "        hist_data_path='/home/oxj7986/Cox_Arrivals/change-point-detection/data_integrated/Simulation_histograms',\n",
    "        N=350, # max number of states\n",
    "        show_histogram=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad4f905",
   "metadata": {},
   "source": [
    "# Z0=5,t=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dce1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #Parameters  for prediction\n",
    "    z_initial=5\n",
    "    min_history_length=10\n",
    "    ws=5\n",
    "    method='cpd'\n",
    "    t=5\n",
    "    values = pd.read_csv(f'/home/oxj7986/Cox_Arrivals/change-point-detection/data_integrated/arrival_data/initial_value_{z_initial}_samples_500.csv')['value']\n",
    "    times = pd.read_csv(f'/home/oxj7986/Cox_Arrivals/change-point-detection/data_integrated/arrival_data/initial_value_{z_initial}_samples_500.csv')['time']\n",
    "    print(f\"{method} Predictor\")\n",
    "    result = rolling_predictor(times, values, min_history_length=min_history_length, verbose=True,method=method,window_size=ws)\n",
    "    file_path_prediction = f'/home/oxj7986/Cox_Arrivals/change-point-detection/data_integrated/result_data/initial_value_{z_initial}_samples_500_predicted_{method}_min_history_length_{min_history_length}_ws_{ws}.csv'\n",
    "    pd.DataFrame(result).to_csv(file_path_prediction)\n",
    "    print(f\"\\nUsage:\")\n",
    "    print(f\"result = rolling_kalman_predictor(data['time'], data['value'])\")\n",
    "    print(f\"All prediction results in: result['detailed_results']\")\n",
    "    print('\\nPrinting PMF overlap:')\n",
    "    file_path_pmf = f'/home/oxj7986/Cox_Arrivals/change-point-detection/data_integrated/pmf_data/M=500/initial_value_{z_initial}_samples_500_predicted_{method}_pmf_min_hist_length_{min_history_length}/'   \n",
    "\n",
    "    print((result['detailed_results']['predicted_step_function_time_interval'].shape),(result['detailed_results']['predicted_step_function'].shape))\n",
    "    plot_pmf_overlap(\n",
    "        [ws],\n",
    "        t=t,\n",
    "        file_path=file_path_prediction,  # changed from 'file' to 'file_path'\n",
    "        save_path=file_path_pmf,\n",
    "        Z_piece=result['detailed_results']['predicted_step_function'],\n",
    "        dt_piece=result['detailed_results']['predicted_step_function_time_interval'],\n",
    "        mu=10 if z_initial == 5 else 100 if z_initial == 80 else 100,\n",
    "        m=1,\n",
    "        z_initial=z_initial,\n",
    "        hist_data_path='/home/oxj7986/Cox_Arrivals/change-point-detection/data_integrated/Simulation_histograms',\n",
    "        N=270, # max number of states\n",
    "        show_histogram=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f850faec",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(42)\n",
    "    z_initial=5\n",
    "    min_history_length=10\n",
    "    method='kalman'\n",
    "    t=5\n",
    "    \n",
    "    values = pd.read_csv(f'/home/oxj7986/Cox_Arrivals/change-point-detection/data_integrated/arrival_data/initial_value_{z_initial}_samples_500.csv')['value']\n",
    "    times = pd.read_csv(f'/home/oxj7986/Cox_Arrivals/change-point-detection/data_integrated/arrival_data/initial_value_{z_initial}_samples_500.csv')['time']\n",
    "    \n",
    "    print(f\"{method}_Predictor\")\n",
    " \n",
    "    # Execute rolling prediction\n",
    "\n",
    "    result = rolling_predictor(times, values, min_history_length=min_history_length, verbose=True,method=method)\n",
    "    file_path_prediction = f'/home/oxj7986/Cox_Arrivals/change-point-detection/data_integrated/result_data/initial_value_{z_initial}_samples_500_predicted_{method}_min_history_length_{min_history_length}.csv'\n",
    "    pd.DataFrame(result).to_csv(file_path_prediction)\n",
    "    \n",
    "    print(f\"\\nUsage:\")\n",
    "    print(f\"result = rolling_kalman_predictor(data['time'], data['value'])\")\n",
    "    print(f\"All prediction results in: result['detailed_results']\")\n",
    "    \n",
    "    print('\\nPrinting PMF overlap:')\n",
    "    file_path_pmf = f'/home/oxj7986/Cox_Arrivals/change-point-detection/data_integrated/pmf_data/M=500/initial_value_{z_initial}_samples_500_predicted_{method}_pmf_min_hist_length_{min_history_length}/'   \n",
    "\n",
    "\n",
    "    plot_pmf_overlap(\n",
    "        [min_history_length],\n",
    "        t=t,\n",
    "        file_path=file_path_prediction,   \n",
    "        save_path=file_path_pmf,\n",
    "        Z_piece=result['detailed_results']['predicted_step_function'],\n",
    "        dt_piece=result['detailed_results']['predicted_step_function_time_interval'],\n",
    "        mu=10 if z_initial == 5 else 100 if z_initial == 80 else 100,\n",
    "        m=1,\n",
    "        z_initial=z_initial,\n",
    "        hist_data_path='/home/oxj7986/Cox_Arrivals/change-point-detection/data_integrated/Simulation_histograms',\n",
    "        N=270, # max number of states\n",
    "        show_histogram=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80335000",
   "metadata": {},
   "source": [
    "# Z0=80,t=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37022093",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(42)\n",
    "    z_initial=80\n",
    "    min_history_length=10\n",
    "    method='kalman'\n",
    "    t=5\n",
    "    \n",
    "    values = pd.read_csv(f'/home/oxj7986/Cox_Arrivals/change-point-detection/data_integrated/arrival_data/initial_value_{z_initial}_samples_500.csv')['value']\n",
    "    times = pd.read_csv(f'/home/oxj7986/Cox_Arrivals/change-point-detection/data_integrated/arrival_data/initial_value_{z_initial}_samples_500.csv')['time']\n",
    "    \n",
    "    print(f\"{method}_Predictor\")\n",
    " \n",
    "    # Execute rolling prediction\n",
    "\n",
    "    result = rolling_predictor(times, values, min_history_length=min_history_length, verbose=True,method=method)\n",
    "    file_path_prediction = f'/home/oxj7986/Cox_Arrivals/change-point-detection/data_integrated/result_data/initial_value_{z_initial}_samples_500_predicted_{method}_min_history_length_{min_history_length}.csv'\n",
    "    pd.DataFrame(result).to_csv(file_path_prediction)\n",
    "    \n",
    "    print(f\"\\nUsage:\")\n",
    "    print(f\"result = rolling_kalman_predictor(data['time'], data['value'])\")\n",
    "    print(f\"All prediction results in: result['detailed_results']\")\n",
    "    \n",
    "    print('\\nPrinting PMF overlap:')\n",
    "    file_path_pmf = f'/home/oxj7986/Cox_Arrivals/change-point-detection/data_integrated/pmf_data/M=500/initial_value_{z_initial}_samples_500_predicted_{method}_pmf_min_hist_length_{min_history_length}/'   \n",
    "\n",
    "\n",
    "    plot_pmf_overlap(\n",
    "        [min_history_length],\n",
    "        t=t,\n",
    "        file_path=file_path_prediction,   \n",
    "        save_path=file_path_pmf,\n",
    "        Z_piece=result['detailed_results']['predicted_step_function'],\n",
    "        dt_piece=result['detailed_results']['predicted_step_function_time_interval'],\n",
    "        mu=10 if z_initial == 5 else 100 if z_initial == 80 else 100,\n",
    "        m=1,\n",
    "        z_initial=z_initial,\n",
    "        hist_data_path='/home/oxj7986/Cox_Arrivals/change-point-detection/data_integrated/Simulation_histograms',\n",
    "        N=100, # max number of states\n",
    "        show_histogram=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607ba574",
   "metadata": {},
   "source": [
    "# Z0=80,t=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b40a606",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(42)\n",
    "    z_initial=80\n",
    "    min_history_length=10\n",
    "    method='kalman'\n",
    "    t=1\n",
    "    \n",
    "    values = pd.read_csv(f'/home/oxj7986/Cox_Arrivals/change-point-detection/data_integrated/arrival_data/initial_value_{z_initial}_samples_500.csv')['value']\n",
    "    times = pd.read_csv(f'/home/oxj7986/Cox_Arrivals/change-point-detection/data_integrated/arrival_data/initial_value_{z_initial}_samples_500.csv')['time']\n",
    "    \n",
    "    print(f\"{method}_Predictor\")\n",
    " \n",
    "    # Execute rolling prediction\n",
    "\n",
    "    result = rolling_predictor(times, values, min_history_length=min_history_length, verbose=True,method=method)\n",
    "    file_path_prediction = f'/home/oxj7986/Cox_Arrivals/change-point-detection/data_integrated/result_data/initial_value_{z_initial}_samples_500_predicted_{method}_min_history_length_{min_history_length}.csv'\n",
    "    pd.DataFrame(result).to_csv(file_path_prediction)\n",
    "    \n",
    "    print(f\"\\nUsage:\")\n",
    "    print(f\"result = rolling_kalman_predictor(data['time'], data['value'])\")\n",
    "    print(f\"All prediction results in: result['detailed_results']\")\n",
    "    \n",
    "    print('\\nPrinting PMF overlap:')\n",
    "    file_path_pmf = f'/home/oxj7986/Cox_Arrivals/change-point-detection/data_integrated/pmf_data/M=500/initial_value_{z_initial}_samples_500_predicted_{method}_pmf_min_hist_length_{min_history_length}/'   \n",
    "\n",
    "\n",
    "    plot_pmf_overlap(\n",
    "        [min_history_length],\n",
    "        t=t,\n",
    "        file_path=file_path_prediction,   \n",
    "        save_path=file_path_pmf,\n",
    "        Z_piece=result['detailed_results']['predicted_step_function'],\n",
    "        dt_piece=result['detailed_results']['predicted_step_function_time_interval'],\n",
    "        mu=10 if z_initial == 5 else 100 if z_initial == 80 else 100,\n",
    "        m=1,\n",
    "        z_initial=z_initial,\n",
    "        hist_data_path='/home/oxj7986/Cox_Arrivals/change-point-detection/data_integrated/Simulation_histograms',\n",
    "        N=100, # max number of states\n",
    "        show_histogram=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427782c2",
   "metadata": {},
   "source": [
    "# Z0=80,t=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ef1012",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #Parameters  for prediction\n",
    "    z_initial=80\n",
    "    min_history_length=10\n",
    "    ws=5\n",
    "    method='cpd'\n",
    "    t=1\n",
    "    values = pd.read_csv(f'/home/oxj7986/Cox_Arrivals/change-point-detection/data_integrated/arrival_data/initial_value_{z_initial}_samples_500.csv')['value']\n",
    "    times = pd.read_csv(f'/home/oxj7986/Cox_Arrivals/change-point-detection/data_integrated/arrival_data/initial_value_{z_initial}_samples_500.csv')['time']\n",
    "    print(f\"{method} Predictor\")\n",
    "    result = rolling_predictor(times, values, min_history_length=min_history_length, verbose=True,method=method,window_size=ws)\n",
    "    file_path_prediction = f'/home/oxj7986/Cox_Arrivals/change-point-detection/data_integrated/result_data/initial_value_{z_initial}_samples_500_predicted_{method}_min_history_length_{min_history_length}_ws_{ws}.csv'\n",
    "    pd.DataFrame(result).to_csv(file_path_prediction)\n",
    "    print(f\"\\nUsage:\")\n",
    "    print(f\"result = rolling_kalman_predictor(data['time'], data['value'])\")\n",
    "    print(f\"All prediction results in: result['detailed_results']\")\n",
    "    print('\\nPrinting PMF overlap:')\n",
    "    file_path_pmf = f'/home/oxj7986/Cox_Arrivals/change-point-detection/data_integrated/pmf_data/M=500/initial_value_{z_initial}_samples_500_predicted_{method}_pmf_min_hist_length_{min_history_length}/'   \n",
    "\n",
    "    print((result['detailed_results']['predicted_step_function_time_interval'].shape),(result['detailed_results']['predicted_step_function'].shape))\n",
    "    plot_pmf_overlap(\n",
    "        [ws],\n",
    "        t=t,\n",
    "        file_path=file_path_prediction,  # changed from 'file' to 'file_path'\n",
    "        save_path=file_path_pmf,\n",
    "        Z_piece=result['detailed_results']['predicted_step_function'],\n",
    "        dt_piece=result['detailed_results']['predicted_step_function_time_interval'],\n",
    "        mu=10 if z_initial == 5 else 100 if z_initial == 80 else 100,\n",
    "        m=1,\n",
    "        z_initial=z_initial,\n",
    "        hist_data_path='/home/oxj7986/Cox_Arrivals/change-point-detection/data_integrated/Simulation_histograms',\n",
    "        N=270, # max number of states\n",
    "        show_histogram=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95659bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #Parameters  for prediction\n",
    "    z_initial=80\n",
    "    min_history_length=10\n",
    "    ws=5\n",
    "    method='cpd'\n",
    "    t=5\n",
    "    values = pd.read_csv(f'/home/oxj7986/Cox_Arrivals/change-point-detection/data_integrated/arrival_data/initial_value_{z_initial}_samples_500.csv')['value']\n",
    "    times = pd.read_csv(f'/home/oxj7986/Cox_Arrivals/change-point-detection/data_integrated/arrival_data/initial_value_{z_initial}_samples_500.csv')['time']\n",
    "    print(f\"{method} Predictor\")\n",
    "    result = rolling_predictor(times, values, min_history_length=min_history_length, verbose=True,method=method,window_size=ws)\n",
    "    file_path_prediction = f'/home/oxj7986/Cox_Arrivals/change-point-detection/data_integrated/result_data/initial_value_{z_initial}_samples_500_predicted_{method}_min_history_length_{min_history_length}_ws_{ws}.csv'\n",
    "    pd.DataFrame(result).to_csv(file_path_prediction)\n",
    "    print(f\"\\nUsage:\")\n",
    "    print(f\"result = rolling_kalman_predictor(data['time'], data['value'])\")\n",
    "    print(f\"All prediction results in: result['detailed_results']\")\n",
    "    print('\\nPrinting PMF overlap:')\n",
    "    file_path_pmf = f'/home/oxj7986/Cox_Arrivals/change-point-detection/data_integrated/pmf_data/M=500/initial_value_{z_initial}_samples_500_predicted_{method}_pmf_min_hist_length_{min_history_length}/'   \n",
    "\n",
    "    print((result['detailed_results']['predicted_step_function_time_interval'].shape),(result['detailed_results']['predicted_step_function'].shape))\n",
    "    plot_pmf_overlap(\n",
    "        [ws],\n",
    "        t=t,\n",
    "        file_path=file_path_prediction,  # changed from 'file' to 'file_path'\n",
    "        save_path=file_path_pmf,\n",
    "        Z_piece=result['detailed_results']['predicted_step_function'],\n",
    "        dt_piece=result['detailed_results']['predicted_step_function_time_interval'],\n",
    "        mu=10 if z_initial == 5 else 100 if z_initial == 80 else 100,\n",
    "        m=1,\n",
    "        z_initial=z_initial,\n",
    "        hist_data_path='/home/oxj7986/Cox_Arrivals/change-point-detection/data_integrated/Simulation_histograms',\n",
    "        N=270, # max number of states\n",
    "        show_histogram=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9997c63f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-virtenv-py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
